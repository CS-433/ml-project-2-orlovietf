{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import spearmanr\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "df = pd.read_csv(\"/home/ml4science0/novozymes/train_updated.csv\")\n",
    "\n",
    "sequences = df[\"protein_sequence\"].tolist()\n",
    "tm = df[\"tm\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, sequences, tm=None):\n",
    "        self.input_ids = sequences[\"input_ids\"]\n",
    "        self.attention_mask = sequences[\"attention_mask\"]\n",
    "        self.tm = tm\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.tm is None:\n",
    "            return {\n",
    "                \"input_ids\": self.input_ids[idx],\n",
    "                \"attention_mask\": self.attention_mask[idx]\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"input_ids\": self.input_ids[idx],\n",
    "                \"attention_mask\": self.attention_mask[idx],\n",
    "                \"tm\": self.tm[idx]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"facebook/esm2_t6_8M_UR50D\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinModel(nn.Module):\n",
    "    def __init__(self, model_checkpoint):\n",
    "        super(ProteinModel, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_checkpoint)\n",
    "        self.fc1 = nn.Linear(320, 120)\n",
    "        self.fc2 = nn.Linear(120, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        cls_token = last_hidden_state[:, 0, :]\n",
    "        out = self.fc1(cls_token)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.ProteinDataset object at 0x7f108f7c7f50>\n",
      "Fold 1/5\n",
      "<torch.utils.data.dataset.Subset object at 0x7f108f84eae0>\n",
      "<torch.utils.data.dataset.Subset object at 0x7f11dc3f5460>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 20:15:24.457738: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734376524.467139 1661470 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734376524.469945 1661470 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-16 20:15:24.479583: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 321.4490\n",
      "Validation Loss: 67.7165\n",
      "Spearman Correlation: 0.5082\n",
      "Epoch 2/5, Train Loss: 64.9673\n",
      "Validation Loss: 61.0064\n",
      "Spearman Correlation: 0.5310\n",
      "Epoch 3/5, Train Loss: 58.7290\n",
      "Validation Loss: 57.1001\n",
      "Spearman Correlation: 0.5488\n",
      "Epoch 4/5, Train Loss: 52.6321\n",
      "Validation Loss: 58.3605\n",
      "Spearman Correlation: 0.5578\n",
      "Epoch 5/5, Train Loss: 47.8951\n",
      "Validation Loss: 55.7869\n",
      "Spearman Correlation: 0.5562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n",
      "<torch.utils.data.dataset.Subset object at 0x7f0ff622c1a0>\n",
      "<torch.utils.data.dataset.Subset object at 0x7f0ff622d9d0>\n",
      "Epoch 1/5, Train Loss: 324.8848\n",
      "Validation Loss: 65.4420\n",
      "Spearman Correlation: 0.5216\n",
      "Epoch 2/5, Train Loss: 62.7491\n",
      "Validation Loss: 60.9669\n",
      "Spearman Correlation: 0.5394\n",
      "Epoch 3/5, Train Loss: 53.6794\n",
      "Validation Loss: 58.9299\n",
      "Spearman Correlation: 0.5615\n",
      "Epoch 4/5, Train Loss: 46.5653\n",
      "Validation Loss: 59.3775\n",
      "Spearman Correlation: 0.5719\n",
      "Epoch 5/5, Train Loss: 37.9396\n",
      "Validation Loss: 57.1369\n",
      "Spearman Correlation: 0.5675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n",
      "<torch.utils.data.dataset.Subset object at 0x7f108255dca0>\n",
      "<torch.utils.data.dataset.Subset object at 0x7f108ec8d4f0>\n",
      "Epoch 1/5, Train Loss: 322.2094\n",
      "Validation Loss: 70.0897\n",
      "Spearman Correlation: 0.5099\n",
      "Epoch 2/5, Train Loss: 62.7953\n",
      "Validation Loss: 63.7072\n",
      "Spearman Correlation: 0.5435\n",
      "Epoch 3/5, Train Loss: 56.7344\n",
      "Validation Loss: 61.4422\n",
      "Spearman Correlation: 0.5622\n",
      "Epoch 4/5, Train Loss: 51.3118\n",
      "Validation Loss: 59.1103\n",
      "Spearman Correlation: 0.5691\n",
      "Epoch 5/5, Train Loss: 45.8820\n",
      "Validation Loss: 60.6027\n",
      "Spearman Correlation: 0.5657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n",
      "<torch.utils.data.dataset.Subset object at 0x7f11dc3f5460>\n",
      "<torch.utils.data.dataset.Subset object at 0x7f108f4cddf0>\n",
      "Epoch 1/5, Train Loss: 319.2952\n",
      "Validation Loss: 68.9296\n",
      "Spearman Correlation: 0.5216\n",
      "Epoch 2/5, Train Loss: 63.0562\n",
      "Validation Loss: 58.3747\n",
      "Spearman Correlation: 0.5595\n",
      "Epoch 3/5, Train Loss: 54.5772\n",
      "Validation Loss: 63.0117\n",
      "Spearman Correlation: 0.5708\n",
      "Epoch 4/5, Train Loss: 47.1118\n",
      "Validation Loss: 58.9907\n",
      "Spearman Correlation: 0.5707\n",
      "Epoch 5/5, Train Loss: 39.1803\n",
      "Validation Loss: 61.7452\n",
      "Spearman Correlation: 0.5545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n",
      "<torch.utils.data.dataset.Subset object at 0x7f108f84eae0>\n",
      "<torch.utils.data.dataset.Subset object at 0x7f10140c3aa0>\n",
      "Epoch 1/5, Train Loss: 316.5728\n",
      "Validation Loss: 69.7140\n",
      "Spearman Correlation: 0.5210\n",
      "Epoch 2/5, Train Loss: 62.7005\n",
      "Validation Loss: 62.2377\n",
      "Spearman Correlation: 0.5490\n",
      "Epoch 3/5, Train Loss: 56.0497\n",
      "Validation Loss: 60.2856\n",
      "Spearman Correlation: 0.5683\n",
      "Epoch 4/5, Train Loss: 50.4222\n",
      "Validation Loss: 59.2929\n",
      "Spearman Correlation: 0.5632\n",
      "Epoch 5/5, Train Loss: 43.6345\n",
      "Validation Loss: 59.7276\n",
      "Spearman Correlation: 0.5572\n",
      "Average Validation Loss: 58.9999\n",
      "Average Spearman Correlation: 0.5602\n"
     ]
    }
   ],
   "source": [
    "# K-Fold Cross Validation\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Tokenize all sequences at once (before splitting)\n",
    "tokenized = tokenizer(sequences, max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "tm_values = torch.tensor(tm, dtype=torch.float32)\n",
    "\n",
    "dataset = ProteinDataset(tokenized, tm_values)\n",
    "\n",
    "predictions_per_fold = []\n",
    "labels_per_fold = []\n",
    "fold_results = []\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f\"Fold {fold + 1}/{k}\")\n",
    "\n",
    "    # Create train and validation subsets\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "    print(train_subset)\n",
    "    print(val_subset)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_subset, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=8, shuffle=False)\n",
    "\n",
    "    # Initialize model, loss, optimizer\n",
    "    model = ProteinModel(model_checkpoint).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    num_epochs = 5\n",
    "    val_losses = []\n",
    "    val_correlations = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Training loop\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            batch_tm = batch[\"tm\"].float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs.squeeze(1), batch_tm)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        total_preds = []\n",
    "        total_tms = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                batch_tm = batch[\"tm\"].float().to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss = criterion(outputs.squeeze(1), batch_tm)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                total_preds += outputs.squeeze(1).cpu().numpy().tolist()\n",
    "                total_tms += batch_tm.cpu().numpy().tolist()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Compute Spearman correlation\n",
    "        spearman_corr = spearmanr(total_preds, total_tms)\n",
    "        val_correlations.append(spearman_corr.correlation)\n",
    "        print(f\"Spearman Correlation: {spearman_corr.correlation:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    total_preds = []\n",
    "    total_tms = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            batch_tm = batch[\"tm\"].float().to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "\n",
    "            total_preds += outputs.squeeze(1).cpu().numpy().tolist()\n",
    "            total_tms += batch_tm.cpu().numpy().tolist()\n",
    "\n",
    "    predictions_per_fold.append(total_preds)\n",
    "    labels_per_fold.append(total_tms)\n",
    "\n",
    "    # Store results for this fold\n",
    "    fold_results.append({\n",
    "        \"fold\": fold + 1,\n",
    "        \"final_val_loss\": val_loss,\n",
    "        \"final_spearman_corr\": val_correlations[-1],\n",
    "    })\n",
    "\n",
    "# Aggregate results across folds\n",
    "avg_val_loss = sum(result[\"final_val_loss\"] for result in fold_results) / k\n",
    "avg_spearman_corr = sum(result[\"final_spearman_corr\"] for result in fold_results) / k\n",
    "\n",
    "print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "print(f\"Average Spearman Correlation: {avg_spearman_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the last element from fold 0 to make all arrays the same length\n",
    "labels_per_fold[0] = labels_per_fold[0][:-1]\n",
    "predictions_per_fold[0] = predictions_per_fold[0][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for i in range(k):\n",
    "    results[f\"tm_{i}\"] = labels_per_fold[i]\n",
    "    results[f\"preds_{i}\"] = predictions_per_fold[i]\n",
    "\n",
    "pd.DataFrame(results).to_csv(\"/home/ml4science0/novozymes/predictions/esm.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

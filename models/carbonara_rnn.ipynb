{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "file_name = '../carbonara_compressed.h5'\n",
    "f = h5py.File(file_name, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "output_tm = np.array(f['output_tm'])\n",
    "input = []\n",
    "for i in range(19149):\n",
    "    input.append(np.array(f[f\"carbonara_z_{i}\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        \"\"\"\n",
    "        :param sequences: List of NumPy arrays, each array is a sequence (matrix of shape [sequence_length, features]).\n",
    "        :param targets: List of target values, one per sequence.\n",
    "        \"\"\"\n",
    "        self.sequences = [torch.tensor(seq, dtype=torch.float32) for seq in sequences]\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets[idx]\n",
    "    \n",
    "dataset = SequenceDataset(input, output_tm)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, targets = zip(*batch)\n",
    "    lengths = torch.tensor([seq.shape[0] for seq in sequences])\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True).to(device)  # Pad sequences to the same length\n",
    "    targets = torch.tensor(targets, dtype=torch.float32).unsqueeze(1)\n",
    "    return padded_sequences, lengths, targets\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Train Loss: 1435.6221, Validation Loss: 730.0390\n",
      "Epoch 2/200, Train Loss: 458.5718, Validation Loss: 301.6448\n",
      "Epoch 3/200, Train Loss: 264.8930, Validation Loss: 244.1045\n",
      "Epoch 4/200, Train Loss: 245.2759, Validation Loss: 239.8449\n",
      "Epoch 5/200, Train Loss: 241.6103, Validation Loss: 236.8275\n",
      "Epoch 6/200, Train Loss: 239.5471, Validation Loss: 234.9616\n",
      "Epoch 7/200, Train Loss: 237.5834, Validation Loss: 234.3267\n",
      "Epoch 8/200, Train Loss: 235.2414, Validation Loss: 229.7176\n",
      "Epoch 9/200, Train Loss: 232.8836, Validation Loss: 228.0556\n",
      "Epoch 10/200, Train Loss: 231.4047, Validation Loss: 226.4322\n",
      "Epoch 11/200, Train Loss: 229.4878, Validation Loss: 223.4454\n",
      "Epoch 12/200, Train Loss: 226.4268, Validation Loss: 220.4719\n",
      "Epoch 13/200, Train Loss: 224.5106, Validation Loss: 219.6126\n",
      "Epoch 14/200, Train Loss: 223.0211, Validation Loss: 220.5961\n",
      "Epoch 15/200, Train Loss: 221.9643, Validation Loss: 216.6128\n",
      "Epoch 16/200, Train Loss: 220.4563, Validation Loss: 216.0991\n",
      "Epoch 17/200, Train Loss: 219.2839, Validation Loss: 215.0134\n",
      "Epoch 18/200, Train Loss: 218.3546, Validation Loss: 214.2817\n",
      "Epoch 19/200, Train Loss: 217.2096, Validation Loss: 213.1862\n",
      "Epoch 20/200, Train Loss: 216.0744, Validation Loss: 211.9483\n",
      "Epoch 21/200, Train Loss: 215.1173, Validation Loss: 210.8332\n",
      "Epoch 22/200, Train Loss: 214.7781, Validation Loss: 210.3784\n",
      "Epoch 23/200, Train Loss: 213.9367, Validation Loss: 209.2058\n",
      "Epoch 24/200, Train Loss: 212.7998, Validation Loss: 209.0264\n",
      "Epoch 25/200, Train Loss: 212.1602, Validation Loss: 209.3882\n",
      "Epoch 26/200, Train Loss: 211.3304, Validation Loss: 207.0903\n",
      "Epoch 27/200, Train Loss: 209.9803, Validation Loss: 206.4022\n",
      "Epoch 28/200, Train Loss: 208.9994, Validation Loss: 205.4996\n",
      "Epoch 29/200, Train Loss: 208.1711, Validation Loss: 204.9224\n",
      "Epoch 30/200, Train Loss: 207.7389, Validation Loss: 204.9456\n",
      "Epoch 31/200, Train Loss: 207.1251, Validation Loss: 203.8171\n",
      "Epoch 32/200, Train Loss: 206.3528, Validation Loss: 202.8860\n",
      "Epoch 33/200, Train Loss: 205.8413, Validation Loss: 202.1862\n",
      "Epoch 34/200, Train Loss: 205.0838, Validation Loss: 202.4727\n",
      "Epoch 35/200, Train Loss: 204.3911, Validation Loss: 201.8880\n",
      "Epoch 36/200, Train Loss: 203.9583, Validation Loss: 201.2539\n",
      "Epoch 37/200, Train Loss: 202.8613, Validation Loss: 202.2091\n",
      "Epoch 38/200, Train Loss: 202.2426, Validation Loss: 200.4266\n",
      "Epoch 39/200, Train Loss: 201.7115, Validation Loss: 199.3028\n",
      "Epoch 40/200, Train Loss: 200.8526, Validation Loss: 199.8769\n",
      "Epoch 41/200, Train Loss: 200.3750, Validation Loss: 200.5398\n",
      "Epoch 42/200, Train Loss: 199.4236, Validation Loss: 198.6424\n",
      "Epoch 43/200, Train Loss: 198.8732, Validation Loss: 197.3429\n",
      "Epoch 44/200, Train Loss: 198.1667, Validation Loss: 197.7745\n",
      "Epoch 45/200, Train Loss: 197.9364, Validation Loss: 196.8780\n",
      "Epoch 46/200, Train Loss: 197.0881, Validation Loss: 202.8564\n",
      "Epoch 47/200, Train Loss: 197.2517, Validation Loss: 195.8364\n",
      "Epoch 48/200, Train Loss: 196.2501, Validation Loss: 195.8041\n",
      "Epoch 49/200, Train Loss: 195.9092, Validation Loss: 196.6238\n",
      "Epoch 50/200, Train Loss: 195.6020, Validation Loss: 197.2502\n",
      "Epoch 51/200, Train Loss: 194.3946, Validation Loss: 195.0936\n",
      "Epoch 52/200, Train Loss: 194.3211, Validation Loss: 194.4414\n",
      "Epoch 53/200, Train Loss: 193.5795, Validation Loss: 195.2352\n",
      "Epoch 54/200, Train Loss: 192.7813, Validation Loss: 194.5837\n",
      "Epoch 55/200, Train Loss: 192.4203, Validation Loss: 193.7674\n",
      "Epoch 56/200, Train Loss: 191.7603, Validation Loss: 195.5716\n",
      "Epoch 57/200, Train Loss: 191.9693, Validation Loss: 193.9512\n",
      "Epoch 58/200, Train Loss: 191.0065, Validation Loss: 193.7244\n",
      "Epoch 59/200, Train Loss: 190.4701, Validation Loss: 195.5452\n",
      "Epoch 60/200, Train Loss: 190.6634, Validation Loss: 193.9684\n",
      "Epoch 61/200, Train Loss: 190.0824, Validation Loss: 193.0893\n",
      "Epoch 62/200, Train Loss: 189.7034, Validation Loss: 193.0635\n",
      "Epoch 63/200, Train Loss: 188.5717, Validation Loss: 194.9356\n",
      "Epoch 64/200, Train Loss: 187.7166, Validation Loss: 193.3729\n",
      "Epoch 65/200, Train Loss: 187.0467, Validation Loss: 193.1928\n",
      "Epoch 66/200, Train Loss: 186.7556, Validation Loss: 192.8167\n",
      "Epoch 67/200, Train Loss: 186.0140, Validation Loss: 192.3719\n",
      "Epoch 68/200, Train Loss: 185.8672, Validation Loss: 192.4505\n",
      "Epoch 69/200, Train Loss: 185.5685, Validation Loss: 191.9487\n",
      "Epoch 70/200, Train Loss: 184.7822, Validation Loss: 192.4612\n",
      "Epoch 71/200, Train Loss: 184.5766, Validation Loss: 194.1457\n",
      "Epoch 72/200, Train Loss: 184.4751, Validation Loss: 192.5478\n",
      "Epoch 73/200, Train Loss: 184.8404, Validation Loss: 191.8704\n",
      "Epoch 74/200, Train Loss: 183.7864, Validation Loss: 191.1530\n",
      "Epoch 75/200, Train Loss: 183.5163, Validation Loss: 192.0384\n",
      "Epoch 76/200, Train Loss: 183.2985, Validation Loss: 192.5277\n",
      "Epoch 77/200, Train Loss: 182.3952, Validation Loss: 191.6900\n",
      "Epoch 78/200, Train Loss: 181.9068, Validation Loss: 191.2710\n",
      "Epoch 79/200, Train Loss: 181.7275, Validation Loss: 192.4585\n",
      "Epoch 80/200, Train Loss: 181.2420, Validation Loss: 190.9498\n",
      "Epoch 81/200, Train Loss: 180.9095, Validation Loss: 192.9977\n",
      "Epoch 82/200, Train Loss: 180.4296, Validation Loss: 192.3131\n",
      "Epoch 83/200, Train Loss: 180.7213, Validation Loss: 191.0497\n",
      "Epoch 84/200, Train Loss: 179.6819, Validation Loss: 193.7497\n",
      "Epoch 85/200, Train Loss: 179.6139, Validation Loss: 191.8141\n",
      "Epoch 86/200, Train Loss: 179.7593, Validation Loss: 192.0097\n",
      "Epoch 87/200, Train Loss: 178.8843, Validation Loss: 193.9052\n",
      "Epoch 88/200, Train Loss: 178.6926, Validation Loss: 192.7474\n",
      "Epoch 89/200, Train Loss: 182.4123, Validation Loss: 195.5026\n",
      "Epoch 90/200, Train Loss: 178.9913, Validation Loss: 194.4888\n",
      "Epoch 91/200, Train Loss: 178.5679, Validation Loss: 195.8957\n",
      "Epoch 92/200, Train Loss: 178.7682, Validation Loss: 191.5889\n",
      "Epoch 93/200, Train Loss: 178.2329, Validation Loss: 193.2745\n",
      "Epoch 94/200, Train Loss: 177.7166, Validation Loss: 191.2494\n",
      "Epoch 95/200, Train Loss: 177.1357, Validation Loss: 192.4183\n",
      "Epoch 96/200, Train Loss: 176.6998, Validation Loss: 191.2097\n",
      "Epoch 97/200, Train Loss: 176.5287, Validation Loss: 192.5452\n",
      "Epoch 98/200, Train Loss: 175.6047, Validation Loss: 192.9275\n",
      "Epoch 99/200, Train Loss: 175.2060, Validation Loss: 191.4790\n",
      "Epoch 100/200, Train Loss: 175.0119, Validation Loss: 192.8183\n",
      "Epoch 101/200, Train Loss: 174.8482, Validation Loss: 192.1642\n",
      "Epoch 102/200, Train Loss: 174.6048, Validation Loss: 192.0533\n",
      "Epoch 103/200, Train Loss: 173.9920, Validation Loss: 191.7099\n",
      "Epoch 104/200, Train Loss: 173.5147, Validation Loss: 191.7073\n",
      "Epoch 105/200, Train Loss: 173.0751, Validation Loss: 192.5974\n",
      "Epoch 106/200, Train Loss: 172.8923, Validation Loss: 193.9576\n",
      "Epoch 107/200, Train Loss: 172.3985, Validation Loss: 192.8063\n",
      "Epoch 108/200, Train Loss: 172.5244, Validation Loss: 192.8891\n",
      "Epoch 109/200, Train Loss: 172.0076, Validation Loss: 192.1665\n",
      "Epoch 110/200, Train Loss: 171.2999, Validation Loss: 192.8801\n",
      "Epoch 111/200, Train Loss: 171.2856, Validation Loss: 191.8142\n",
      "Epoch 112/200, Train Loss: 170.6955, Validation Loss: 192.7674\n",
      "Epoch 113/200, Train Loss: 170.3764, Validation Loss: 193.9473\n",
      "Epoch 114/200, Train Loss: 170.5558, Validation Loss: 195.1638\n",
      "Epoch 115/200, Train Loss: 169.9450, Validation Loss: 193.2408\n",
      "Epoch 116/200, Train Loss: 169.6307, Validation Loss: 192.8132\n",
      "Epoch 117/200, Train Loss: 169.4061, Validation Loss: 192.8367\n",
      "Epoch 118/200, Train Loss: 168.5808, Validation Loss: 191.6972\n",
      "Epoch 119/200, Train Loss: 168.6612, Validation Loss: 192.8262\n",
      "Epoch 120/200, Train Loss: 168.1868, Validation Loss: 194.6566\n",
      "Epoch 121/200, Train Loss: 168.3063, Validation Loss: 193.7360\n",
      "Epoch 122/200, Train Loss: 167.8669, Validation Loss: 193.0571\n",
      "Epoch 123/200, Train Loss: 167.0447, Validation Loss: 193.7959\n",
      "Epoch 124/200, Train Loss: 167.1005, Validation Loss: 193.9555\n",
      "Epoch 125/200, Train Loss: 166.5479, Validation Loss: 192.3829\n",
      "Epoch 126/200, Train Loss: 166.8891, Validation Loss: 193.0826\n",
      "Epoch 127/200, Train Loss: 166.6946, Validation Loss: 191.6562\n",
      "Epoch 128/200, Train Loss: 166.4518, Validation Loss: 193.8477\n",
      "Epoch 129/200, Train Loss: 166.1914, Validation Loss: 194.3378\n",
      "Epoch 130/200, Train Loss: 165.0499, Validation Loss: 194.4941\n",
      "Epoch 131/200, Train Loss: 165.2605, Validation Loss: 194.1891\n",
      "Epoch 132/200, Train Loss: 164.3213, Validation Loss: 197.2636\n",
      "Epoch 133/200, Train Loss: 164.7730, Validation Loss: 193.8027\n",
      "Epoch 134/200, Train Loss: 164.6354, Validation Loss: 195.1366\n",
      "Epoch 135/200, Train Loss: 163.7282, Validation Loss: 193.9704\n",
      "Epoch 136/200, Train Loss: 163.8586, Validation Loss: 193.3279\n",
      "Epoch 137/200, Train Loss: 163.3619, Validation Loss: 197.7730\n",
      "Epoch 138/200, Train Loss: 162.8714, Validation Loss: 195.3991\n",
      "Epoch 139/200, Train Loss: 162.8016, Validation Loss: 194.1592\n",
      "Epoch 140/200, Train Loss: 162.6400, Validation Loss: 194.2639\n",
      "Epoch 141/200, Train Loss: 162.7124, Validation Loss: 194.9588\n",
      "Epoch 142/200, Train Loss: 162.2754, Validation Loss: 193.8119\n",
      "Epoch 143/200, Train Loss: 162.0897, Validation Loss: 198.2047\n",
      "Epoch 144/200, Train Loss: 161.6810, Validation Loss: 194.2662\n",
      "Epoch 145/200, Train Loss: 161.1583, Validation Loss: 195.0825\n",
      "Epoch 146/200, Train Loss: 160.4448, Validation Loss: 195.6053\n",
      "Epoch 147/200, Train Loss: 160.8542, Validation Loss: 197.1989\n",
      "Epoch 148/200, Train Loss: 160.5617, Validation Loss: 195.0333\n",
      "Epoch 149/200, Train Loss: 160.5928, Validation Loss: 193.7903\n",
      "Epoch 150/200, Train Loss: 161.1329, Validation Loss: 195.6761\n",
      "Epoch 151/200, Train Loss: 160.2002, Validation Loss: 198.0471\n",
      "Epoch 152/200, Train Loss: 159.4925, Validation Loss: 194.7198\n",
      "Epoch 153/200, Train Loss: 159.4874, Validation Loss: 197.7041\n",
      "Epoch 154/200, Train Loss: 158.3154, Validation Loss: 195.2601\n",
      "Epoch 155/200, Train Loss: 159.1842, Validation Loss: 195.5193\n",
      "Epoch 156/200, Train Loss: 158.9884, Validation Loss: 196.4914\n",
      "Epoch 157/200, Train Loss: 159.4462, Validation Loss: 195.2671\n",
      "Epoch 158/200, Train Loss: 158.4689, Validation Loss: 196.1086\n",
      "Epoch 159/200, Train Loss: 158.2024, Validation Loss: 195.1731\n",
      "Epoch 160/200, Train Loss: 158.0127, Validation Loss: 196.2478\n",
      "Epoch 161/200, Train Loss: 157.3803, Validation Loss: 198.9608\n",
      "Epoch 162/200, Train Loss: 157.3069, Validation Loss: 196.1410\n",
      "Epoch 163/200, Train Loss: 156.6677, Validation Loss: 198.6231\n",
      "Epoch 164/200, Train Loss: 156.3620, Validation Loss: 200.3328\n",
      "Epoch 165/200, Train Loss: 156.8301, Validation Loss: 197.5706\n",
      "Epoch 166/200, Train Loss: 156.3183, Validation Loss: 203.3127\n",
      "Epoch 167/200, Train Loss: 156.3302, Validation Loss: 197.1824\n",
      "Epoch 168/200, Train Loss: 155.6390, Validation Loss: 196.9678\n",
      "Epoch 169/200, Train Loss: 155.9833, Validation Loss: 197.5631\n",
      "Epoch 170/200, Train Loss: 155.3561, Validation Loss: 196.7722\n",
      "Epoch 171/200, Train Loss: 155.5856, Validation Loss: 196.5600\n",
      "Epoch 172/200, Train Loss: 155.1149, Validation Loss: 199.7111\n",
      "Epoch 173/200, Train Loss: 154.6379, Validation Loss: 199.0021\n",
      "Epoch 174/200, Train Loss: 154.4248, Validation Loss: 198.0091\n",
      "Epoch 175/200, Train Loss: 153.9235, Validation Loss: 198.4824\n",
      "Epoch 176/200, Train Loss: 154.8942, Validation Loss: 199.8292\n",
      "Epoch 177/200, Train Loss: 154.5813, Validation Loss: 201.0881\n",
      "Epoch 178/200, Train Loss: 153.1080, Validation Loss: 197.4258\n",
      "Epoch 179/200, Train Loss: 153.4531, Validation Loss: 198.3009\n",
      "Epoch 180/200, Train Loss: 153.4507, Validation Loss: 200.3985\n",
      "Epoch 181/200, Train Loss: 153.6327, Validation Loss: 201.5289\n",
      "Epoch 182/200, Train Loss: 152.8072, Validation Loss: 201.2222\n",
      "Epoch 183/200, Train Loss: 152.6342, Validation Loss: 198.9609\n",
      "Epoch 184/200, Train Loss: 152.6275, Validation Loss: 199.2391\n",
      "Epoch 185/200, Train Loss: 152.5576, Validation Loss: 198.8399\n",
      "Epoch 186/200, Train Loss: 152.5397, Validation Loss: 199.4964\n",
      "Epoch 187/200, Train Loss: 152.1035, Validation Loss: 199.8296\n",
      "Epoch 188/200, Train Loss: 151.4445, Validation Loss: 199.3885\n",
      "Epoch 189/200, Train Loss: 151.5239, Validation Loss: 198.9020\n",
      "Epoch 190/200, Train Loss: 150.9402, Validation Loss: 200.1521\n",
      "Epoch 191/200, Train Loss: 151.2614, Validation Loss: 202.7241\n",
      "Epoch 192/200, Train Loss: 150.5070, Validation Loss: 202.4948\n",
      "Epoch 193/200, Train Loss: 150.6461, Validation Loss: 202.7660\n",
      "Epoch 194/200, Train Loss: 150.5489, Validation Loss: 200.6977\n",
      "Epoch 195/200, Train Loss: 150.6390, Validation Loss: 200.2493\n",
      "Epoch 196/200, Train Loss: 150.2294, Validation Loss: 199.4335\n",
      "Epoch 197/200, Train Loss: 150.8141, Validation Loss: 199.6418\n",
      "Epoch 198/200, Train Loss: 150.1733, Validation Loss: 200.5355\n",
      "Epoch 199/200, Train Loss: 150.6490, Validation Loss: 200.6739\n",
      "Epoch 200/200, Train Loss: 149.5096, Validation Loss: 202.6311\n"
     ]
    }
   ],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)  # LSTM layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)  # Fully connected layer for output\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        # Pack the padded sequences\n",
    "        packed_input = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hidden, _) = self.rnn(packed_input)  # RNN output\n",
    "        \n",
    "        # Use the last hidden state\n",
    "        output = self.fc(hidden[-1])  # Take the last layer of the hidden state\n",
    "        return output\n",
    "    \n",
    "input_size = input[0].shape[1]  # Number of features\n",
    "hidden_size = 64\n",
    "output_size = 1  # Regression target\n",
    "\n",
    "model = RNNModel(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Step 3: Loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Step 4: Train the Model\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_X, lengths, batch_y in train_loader:\n",
    "        predictions = model(batch_X, lengths)\n",
    "        loss = loss_fn(predictions, batch_y.to(device))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validate the Model\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, lengths, batch_y in val_loader:\n",
    "            predictions = model(batch_X, lengths)\n",
    "            loss = loss_fn(predictions, batch_y.to(device))\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Validation Loss: {val_loss/len(val_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Correlation: 0.3139\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Compute correlation loss on validation set\n",
    "model.eval()  # Ensure model is in evaluation mode\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch_X, lengths, batch_y in val_loader:\n",
    "        predictions = model(batch_X, lengths)\n",
    "        all_predictions.append(predictions)\n",
    "        all_labels.append(batch_y.to(device))\n",
    "\n",
    "all_predictions = torch.cat(all_predictions, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "val_corr = spearmanr(all_predictions.cpu(), all_labels.cpu()).correlation\n",
    "print(f\"Validation Correlation: {val_corr:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

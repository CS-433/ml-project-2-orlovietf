{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "file_name = '../carbonara_compressed.h5'\n",
    "f = h5py.File(file_name, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../train_updated.csv\")\n",
    "protein_sequences = df[\"protein_sequence\"].values\n",
    "seq_ids = df[\"seq_id\"].values\n",
    "tm = df[\"tm\"].values\n",
    "output_tm = {}\n",
    "for i in range(len(tm)):\n",
    "    output_tm[protein_sequences[i]]=tm[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sequences = f[\"sequences\"]\n",
    "filtered_tm = []\n",
    "input = []\n",
    "for i in range(19149):\n",
    "    if sequences[i].decode('utf-8') not in protein_sequences:\n",
    "        # Skip sequences removed in the train_updated.csv\n",
    "        continue\n",
    "    input.append(np.array(f[f\"carbonara_z_{i}\"]))\n",
    "    filtered_tm.append(output_tm[sequences[i].decode('utf-8')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        \"\"\"\n",
    "        :param sequences: List of NumPy arrays, each array is a sequence (matrix of shape [sequence_length, features]).\n",
    "        :param targets: List of target values, one per sequence.\n",
    "        \"\"\"\n",
    "        self.sequences = [torch.tensor(seq, dtype=torch.float32) for seq in sequences]\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets[idx]\n",
    "    \n",
    "dataset = SequenceDataset(input, filtered_tm)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, targets = zip(*batch)\n",
    "    lengths = torch.tensor([seq.shape[0] for seq in sequences])\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True).to(device)  # Pad sequences to the same length\n",
    "    targets = torch.tensor(targets, dtype=torch.float32).unsqueeze(1)\n",
    "    return padded_sequences, lengths, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch 1/100, Train Loss: 1679.7865, Validation Loss: 862.3233\n",
      "Epoch 2/100, Train Loss: 469.3542, Validation Loss: 249.8520\n",
      "Epoch 3/100, Train Loss: 197.9593, Validation Loss: 167.7333\n",
      "Epoch 4/100, Train Loss: 170.1092, Validation Loss: 163.9583\n",
      "Epoch 5/100, Train Loss: 168.8528, Validation Loss: 163.9257\n",
      "Epoch 6/100, Train Loss: 169.0677, Validation Loss: 163.9106\n",
      "Epoch 7/100, Train Loss: 168.6483, Validation Loss: 163.9128\n",
      "Epoch 8/100, Train Loss: 168.8901, Validation Loss: 162.5688\n",
      "Epoch 9/100, Train Loss: 167.5064, Validation Loss: 161.8030\n",
      "Epoch 10/100, Train Loss: 166.5852, Validation Loss: 161.0312\n",
      "Epoch 11/100, Train Loss: 165.9932, Validation Loss: 160.1927\n",
      "Epoch 12/100, Train Loss: 165.5084, Validation Loss: 160.3042\n",
      "Epoch 13/100, Train Loss: 164.8084, Validation Loss: 159.2007\n",
      "Epoch 14/100, Train Loss: 164.0223, Validation Loss: 158.5847\n",
      "Epoch 15/100, Train Loss: 163.3085, Validation Loss: 158.3607\n",
      "Epoch 16/100, Train Loss: 162.8345, Validation Loss: 157.6304\n",
      "Epoch 17/100, Train Loss: 162.3195, Validation Loss: 157.7658\n",
      "Epoch 18/100, Train Loss: 162.4277, Validation Loss: 157.4121\n",
      "Epoch 19/100, Train Loss: 161.3383, Validation Loss: 156.6915\n",
      "Epoch 20/100, Train Loss: 160.7086, Validation Loss: 157.6235\n",
      "Epoch 21/100, Train Loss: 160.2406, Validation Loss: 155.0130\n",
      "Epoch 22/100, Train Loss: 159.6000, Validation Loss: 155.3238\n",
      "Epoch 23/100, Train Loss: 159.4268, Validation Loss: 154.2959\n",
      "Epoch 24/100, Train Loss: 158.8426, Validation Loss: 154.6022\n",
      "Epoch 25/100, Train Loss: 158.1747, Validation Loss: 155.3163\n",
      "Epoch 26/100, Train Loss: 157.9833, Validation Loss: 154.6553\n",
      "Epoch 27/100, Train Loss: 157.6231, Validation Loss: 154.0237\n",
      "Epoch 28/100, Train Loss: 156.8932, Validation Loss: 154.8681\n",
      "Epoch 29/100, Train Loss: 156.4707, Validation Loss: 153.1711\n",
      "Epoch 30/100, Train Loss: 156.2016, Validation Loss: 153.1769\n",
      "Epoch 31/100, Train Loss: 156.2265, Validation Loss: 152.6630\n",
      "Epoch 32/100, Train Loss: 155.9389, Validation Loss: 152.8219\n",
      "Epoch 33/100, Train Loss: 155.2865, Validation Loss: 152.7416\n",
      "Epoch 34/100, Train Loss: 154.8636, Validation Loss: 152.1569\n",
      "Epoch 35/100, Train Loss: 154.6901, Validation Loss: 152.7680\n",
      "Epoch 36/100, Train Loss: 154.0430, Validation Loss: 151.8558\n",
      "Epoch 37/100, Train Loss: 153.9118, Validation Loss: 151.7915\n",
      "Epoch 38/100, Train Loss: 154.2786, Validation Loss: 152.9252\n",
      "Epoch 39/100, Train Loss: 153.2883, Validation Loss: 151.2436\n",
      "Epoch 40/100, Train Loss: 152.6845, Validation Loss: 151.7473\n",
      "Epoch 41/100, Train Loss: 152.7236, Validation Loss: 151.3856\n",
      "Epoch 42/100, Train Loss: 152.6020, Validation Loss: 152.1006\n",
      "Epoch 43/100, Train Loss: 151.9896, Validation Loss: 150.9971\n",
      "Epoch 44/100, Train Loss: 151.5669, Validation Loss: 151.0031\n",
      "Epoch 45/100, Train Loss: 151.5786, Validation Loss: 150.6843\n",
      "Epoch 46/100, Train Loss: 150.8977, Validation Loss: 151.4853\n",
      "Epoch 47/100, Train Loss: 150.5443, Validation Loss: 150.6842\n",
      "Epoch 48/100, Train Loss: 150.2259, Validation Loss: 150.6959\n",
      "Epoch 49/100, Train Loss: 150.2159, Validation Loss: 150.9952\n",
      "Epoch 50/100, Train Loss: 149.8012, Validation Loss: 151.3133\n",
      "Epoch 51/100, Train Loss: 149.2974, Validation Loss: 151.2630\n",
      "Epoch 52/100, Train Loss: 149.1651, Validation Loss: 150.3047\n",
      "Epoch 53/100, Train Loss: 148.8233, Validation Loss: 153.3535\n",
      "Epoch 54/100, Train Loss: 148.6809, Validation Loss: 150.9653\n",
      "Epoch 55/100, Train Loss: 148.3206, Validation Loss: 150.8891\n",
      "Epoch 56/100, Train Loss: 147.7909, Validation Loss: 150.9082\n",
      "Epoch 57/100, Train Loss: 147.4142, Validation Loss: 150.2269\n",
      "Epoch 58/100, Train Loss: 146.7554, Validation Loss: 152.4801\n",
      "Epoch 59/100, Train Loss: 146.5052, Validation Loss: 150.6290\n",
      "Epoch 60/100, Train Loss: 146.4209, Validation Loss: 151.2946\n",
      "Epoch 61/100, Train Loss: 146.1277, Validation Loss: 150.0251\n",
      "Epoch 62/100, Train Loss: 145.6796, Validation Loss: 149.8126\n",
      "Epoch 63/100, Train Loss: 145.6116, Validation Loss: 149.9242\n",
      "Epoch 64/100, Train Loss: 145.1066, Validation Loss: 149.7135\n",
      "Epoch 65/100, Train Loss: 144.2383, Validation Loss: 149.5032\n",
      "Epoch 66/100, Train Loss: 144.7079, Validation Loss: 149.8352\n",
      "Epoch 67/100, Train Loss: 144.5101, Validation Loss: 150.1476\n",
      "Epoch 68/100, Train Loss: 143.5875, Validation Loss: 150.3462\n",
      "Epoch 69/100, Train Loss: 143.3456, Validation Loss: 149.4874\n",
      "Epoch 70/100, Train Loss: 142.9907, Validation Loss: 149.9269\n",
      "Epoch 71/100, Train Loss: 142.8361, Validation Loss: 150.5832\n",
      "Epoch 72/100, Train Loss: 142.1517, Validation Loss: 150.2456\n",
      "Epoch 73/100, Train Loss: 141.8410, Validation Loss: 151.2882\n",
      "Epoch 74/100, Train Loss: 141.9390, Validation Loss: 150.2503\n",
      "Epoch 75/100, Train Loss: 142.0883, Validation Loss: 150.3935\n",
      "Epoch 76/100, Train Loss: 140.9783, Validation Loss: 150.2636\n",
      "Epoch 77/100, Train Loss: 140.9571, Validation Loss: 150.7082\n",
      "Epoch 78/100, Train Loss: 141.0283, Validation Loss: 150.9820\n",
      "Epoch 79/100, Train Loss: 141.3264, Validation Loss: 150.6073\n",
      "Epoch 80/100, Train Loss: 140.3744, Validation Loss: 152.2162\n",
      "Epoch 81/100, Train Loss: 139.7404, Validation Loss: 150.5980\n",
      "Epoch 82/100, Train Loss: 139.2804, Validation Loss: 150.3500\n",
      "Epoch 83/100, Train Loss: 139.2930, Validation Loss: 150.0486\n",
      "Epoch 84/100, Train Loss: 139.1818, Validation Loss: 150.5016\n",
      "Epoch 85/100, Train Loss: 138.2762, Validation Loss: 149.9731\n",
      "Epoch 86/100, Train Loss: 137.8397, Validation Loss: 150.5312\n",
      "Epoch 87/100, Train Loss: 137.3029, Validation Loss: 151.1745\n",
      "Epoch 88/100, Train Loss: 136.9099, Validation Loss: 150.7648\n",
      "Epoch 89/100, Train Loss: 136.7436, Validation Loss: 152.1684\n",
      "Epoch 90/100, Train Loss: 136.8124, Validation Loss: 149.8124\n",
      "Epoch 91/100, Train Loss: 135.9700, Validation Loss: 151.1130\n",
      "Epoch 92/100, Train Loss: 135.8297, Validation Loss: 150.1169\n",
      "Epoch 93/100, Train Loss: 135.8168, Validation Loss: 151.2639\n",
      "Epoch 94/100, Train Loss: 135.7076, Validation Loss: 151.0477\n",
      "Epoch 95/100, Train Loss: 135.3940, Validation Loss: 151.3751\n",
      "Epoch 96/100, Train Loss: 134.2129, Validation Loss: 150.6915\n",
      "Epoch 97/100, Train Loss: 134.2544, Validation Loss: 150.9670\n",
      "Epoch 98/100, Train Loss: 134.4562, Validation Loss: 152.2147\n",
      "Epoch 99/100, Train Loss: 133.8629, Validation Loss: 151.4142\n",
      "Epoch 100/100, Train Loss: 133.3850, Validation Loss: 152.0713\n",
      "Fold 2/5\n",
      "Epoch 1/100, Train Loss: 1692.5897, Validation Loss: 866.6466\n",
      "Epoch 2/100, Train Loss: 453.5781, Validation Loss: 234.2321\n",
      "Epoch 3/100, Train Loss: 188.7814, Validation Loss: 173.3076\n",
      "Epoch 4/100, Train Loss: 167.8099, Validation Loss: 170.9617\n",
      "Epoch 5/100, Train Loss: 166.9804, Validation Loss: 170.9875\n",
      "Epoch 6/100, Train Loss: 166.9254, Validation Loss: 170.9651\n",
      "Epoch 7/100, Train Loss: 167.1178, Validation Loss: 171.0811\n",
      "Epoch 8/100, Train Loss: 167.0518, Validation Loss: 170.3402\n",
      "Epoch 9/100, Train Loss: 166.0108, Validation Loss: 169.5330\n",
      "Epoch 10/100, Train Loss: 164.6449, Validation Loss: 168.8505\n",
      "Epoch 11/100, Train Loss: 163.9426, Validation Loss: 168.5133\n",
      "Epoch 12/100, Train Loss: 162.7374, Validation Loss: 167.9028\n",
      "Epoch 13/100, Train Loss: 162.5892, Validation Loss: 167.3558\n",
      "Epoch 14/100, Train Loss: 161.9972, Validation Loss: 166.6661\n",
      "Epoch 15/100, Train Loss: 161.1180, Validation Loss: 166.4552\n",
      "Epoch 16/100, Train Loss: 160.8466, Validation Loss: 165.9127\n",
      "Epoch 17/100, Train Loss: 160.2855, Validation Loss: 165.7988\n",
      "Epoch 18/100, Train Loss: 159.5184, Validation Loss: 166.0094\n",
      "Epoch 19/100, Train Loss: 159.4593, Validation Loss: 165.4140\n",
      "Epoch 20/100, Train Loss: 158.9811, Validation Loss: 165.6033\n",
      "Epoch 21/100, Train Loss: 158.1358, Validation Loss: 164.8382\n",
      "Epoch 22/100, Train Loss: 157.9785, Validation Loss: 164.8370\n",
      "Epoch 23/100, Train Loss: 157.0852, Validation Loss: 164.0236\n",
      "Epoch 24/100, Train Loss: 156.8165, Validation Loss: 164.1785\n",
      "Epoch 25/100, Train Loss: 156.4944, Validation Loss: 163.7304\n",
      "Epoch 26/100, Train Loss: 155.8305, Validation Loss: 163.9642\n",
      "Epoch 27/100, Train Loss: 155.3479, Validation Loss: 163.7252\n",
      "Epoch 28/100, Train Loss: 155.0483, Validation Loss: 162.7833\n",
      "Epoch 29/100, Train Loss: 154.9846, Validation Loss: 163.5370\n",
      "Epoch 30/100, Train Loss: 154.5746, Validation Loss: 162.8544\n",
      "Epoch 31/100, Train Loss: 154.5762, Validation Loss: 162.9513\n",
      "Epoch 32/100, Train Loss: 153.7791, Validation Loss: 163.5490\n",
      "Epoch 33/100, Train Loss: 153.4713, Validation Loss: 164.2113\n",
      "Epoch 34/100, Train Loss: 153.0649, Validation Loss: 162.6829\n",
      "Epoch 35/100, Train Loss: 152.9975, Validation Loss: 162.3618\n",
      "Epoch 36/100, Train Loss: 152.0603, Validation Loss: 162.6751\n",
      "Epoch 37/100, Train Loss: 152.2960, Validation Loss: 162.1139\n",
      "Epoch 38/100, Train Loss: 152.3333, Validation Loss: 161.6344\n",
      "Epoch 39/100, Train Loss: 151.8947, Validation Loss: 162.0279\n",
      "Epoch 40/100, Train Loss: 151.5730, Validation Loss: 161.6837\n",
      "Epoch 41/100, Train Loss: 151.0445, Validation Loss: 161.3068\n",
      "Epoch 42/100, Train Loss: 150.7376, Validation Loss: 161.4686\n",
      "Epoch 43/100, Train Loss: 150.7150, Validation Loss: 161.9304\n",
      "Epoch 44/100, Train Loss: 150.2771, Validation Loss: 161.6715\n",
      "Epoch 45/100, Train Loss: 149.9628, Validation Loss: 160.9744\n",
      "Epoch 46/100, Train Loss: 149.3016, Validation Loss: 160.6328\n",
      "Epoch 47/100, Train Loss: 149.1885, Validation Loss: 160.8801\n",
      "Epoch 48/100, Train Loss: 148.9389, Validation Loss: 160.5879\n",
      "Epoch 49/100, Train Loss: 148.4333, Validation Loss: 160.4300\n",
      "Epoch 50/100, Train Loss: 147.7352, Validation Loss: 160.2186\n",
      "Epoch 51/100, Train Loss: 147.7116, Validation Loss: 161.0843\n",
      "Epoch 52/100, Train Loss: 147.8155, Validation Loss: 160.3496\n",
      "Epoch 53/100, Train Loss: 147.6716, Validation Loss: 160.8694\n",
      "Epoch 54/100, Train Loss: 147.5632, Validation Loss: 159.8498\n",
      "Epoch 55/100, Train Loss: 146.7555, Validation Loss: 160.1635\n",
      "Epoch 56/100, Train Loss: 146.6328, Validation Loss: 160.9257\n",
      "Epoch 57/100, Train Loss: 146.2848, Validation Loss: 159.5144\n",
      "Epoch 58/100, Train Loss: 145.7526, Validation Loss: 159.4074\n",
      "Epoch 59/100, Train Loss: 145.2874, Validation Loss: 161.0209\n",
      "Epoch 60/100, Train Loss: 144.4851, Validation Loss: 159.6291\n",
      "Epoch 61/100, Train Loss: 144.8142, Validation Loss: 160.7470\n",
      "Epoch 62/100, Train Loss: 145.2158, Validation Loss: 159.2871\n",
      "Epoch 63/100, Train Loss: 144.4417, Validation Loss: 159.5656\n",
      "Epoch 64/100, Train Loss: 145.0391, Validation Loss: 159.1262\n",
      "Epoch 65/100, Train Loss: 143.7456, Validation Loss: 159.4586\n",
      "Epoch 66/100, Train Loss: 143.4822, Validation Loss: 159.4006\n",
      "Epoch 67/100, Train Loss: 143.5140, Validation Loss: 161.1195\n",
      "Epoch 68/100, Train Loss: 143.2157, Validation Loss: 159.5750\n",
      "Epoch 69/100, Train Loss: 142.4125, Validation Loss: 159.0016\n",
      "Epoch 70/100, Train Loss: 142.2884, Validation Loss: 159.7317\n",
      "Epoch 71/100, Train Loss: 142.3353, Validation Loss: 159.5686\n",
      "Epoch 72/100, Train Loss: 141.6946, Validation Loss: 159.4929\n",
      "Epoch 73/100, Train Loss: 141.0716, Validation Loss: 159.1284\n",
      "Epoch 74/100, Train Loss: 140.6598, Validation Loss: 159.5870\n",
      "Epoch 75/100, Train Loss: 140.3334, Validation Loss: 159.6210\n",
      "Epoch 76/100, Train Loss: 140.3539, Validation Loss: 158.9198\n",
      "Epoch 77/100, Train Loss: 140.3262, Validation Loss: 158.9751\n",
      "Epoch 78/100, Train Loss: 139.6756, Validation Loss: 159.0585\n",
      "Epoch 79/100, Train Loss: 139.2622, Validation Loss: 159.4686\n",
      "Epoch 80/100, Train Loss: 139.2747, Validation Loss: 159.7468\n",
      "Epoch 81/100, Train Loss: 138.8640, Validation Loss: 161.6787\n",
      "Epoch 82/100, Train Loss: 138.6142, Validation Loss: 159.3837\n",
      "Epoch 83/100, Train Loss: 138.7462, Validation Loss: 159.6772\n",
      "Epoch 84/100, Train Loss: 138.3696, Validation Loss: 159.0687\n",
      "Epoch 85/100, Train Loss: 138.0265, Validation Loss: 159.9192\n",
      "Epoch 86/100, Train Loss: 137.6607, Validation Loss: 160.5162\n",
      "Epoch 87/100, Train Loss: 137.2249, Validation Loss: 159.7219\n",
      "Epoch 88/100, Train Loss: 137.3918, Validation Loss: 160.0409\n",
      "Epoch 89/100, Train Loss: 136.3789, Validation Loss: 159.4749\n",
      "Epoch 90/100, Train Loss: 135.6397, Validation Loss: 161.8010\n",
      "Epoch 91/100, Train Loss: 135.6649, Validation Loss: 159.4390\n",
      "Epoch 92/100, Train Loss: 135.9225, Validation Loss: 160.8576\n",
      "Epoch 93/100, Train Loss: 135.8045, Validation Loss: 159.8036\n",
      "Epoch 94/100, Train Loss: 135.0824, Validation Loss: 165.9151\n",
      "Epoch 95/100, Train Loss: 134.7765, Validation Loss: 159.8779\n",
      "Epoch 96/100, Train Loss: 134.7213, Validation Loss: 161.1717\n",
      "Epoch 97/100, Train Loss: 133.7623, Validation Loss: 160.6559\n",
      "Epoch 98/100, Train Loss: 134.2732, Validation Loss: 159.9345\n",
      "Epoch 99/100, Train Loss: 133.3350, Validation Loss: 159.7481\n",
      "Epoch 100/100, Train Loss: 133.1772, Validation Loss: 161.0850\n",
      "Fold 3/5\n",
      "Epoch 1/100, Train Loss: 1643.7612, Validation Loss: 927.4065\n",
      "Epoch 2/100, Train Loss: 546.0895, Validation Loss: 318.2504\n",
      "Epoch 3/100, Train Loss: 222.1020, Validation Loss: 186.1627\n",
      "Epoch 4/100, Train Loss: 170.4568, Validation Loss: 173.2636\n",
      "Epoch 5/100, Train Loss: 167.2232, Validation Loss: 172.5490\n",
      "Epoch 6/100, Train Loss: 167.0511, Validation Loss: 172.5827\n",
      "Epoch 7/100, Train Loss: 166.6972, Validation Loss: 172.4983\n",
      "Epoch 8/100, Train Loss: 166.9112, Validation Loss: 172.0477\n",
      "Epoch 9/100, Train Loss: 165.6094, Validation Loss: 170.2296\n",
      "Epoch 10/100, Train Loss: 164.4260, Validation Loss: 169.8370\n",
      "Epoch 11/100, Train Loss: 163.2166, Validation Loss: 168.8787\n",
      "Epoch 12/100, Train Loss: 162.9933, Validation Loss: 167.9144\n",
      "Epoch 13/100, Train Loss: 162.2418, Validation Loss: 167.8912\n",
      "Epoch 14/100, Train Loss: 162.0814, Validation Loss: 166.9292\n",
      "Epoch 15/100, Train Loss: 160.9411, Validation Loss: 166.5092\n",
      "Epoch 16/100, Train Loss: 160.3135, Validation Loss: 166.1368\n",
      "Epoch 17/100, Train Loss: 160.0831, Validation Loss: 165.8792\n",
      "Epoch 18/100, Train Loss: 159.2571, Validation Loss: 166.1322\n",
      "Epoch 19/100, Train Loss: 158.8195, Validation Loss: 165.5654\n",
      "Epoch 20/100, Train Loss: 158.7265, Validation Loss: 165.3354\n",
      "Epoch 21/100, Train Loss: 157.4690, Validation Loss: 164.7308\n",
      "Epoch 22/100, Train Loss: 156.9721, Validation Loss: 164.3651\n",
      "Epoch 23/100, Train Loss: 157.3129, Validation Loss: 164.2225\n",
      "Epoch 24/100, Train Loss: 156.2709, Validation Loss: 164.5004\n",
      "Epoch 25/100, Train Loss: 155.5723, Validation Loss: 164.5621\n",
      "Epoch 26/100, Train Loss: 155.3359, Validation Loss: 163.5493\n",
      "Epoch 27/100, Train Loss: 155.1675, Validation Loss: 163.5108\n",
      "Epoch 28/100, Train Loss: 155.2751, Validation Loss: 163.6394\n",
      "Epoch 29/100, Train Loss: 154.6327, Validation Loss: 163.4760\n",
      "Epoch 30/100, Train Loss: 154.2185, Validation Loss: 163.7959\n",
      "Epoch 31/100, Train Loss: 153.9678, Validation Loss: 163.5264\n",
      "Epoch 32/100, Train Loss: 153.4565, Validation Loss: 163.4447\n",
      "Epoch 33/100, Train Loss: 153.3203, Validation Loss: 163.1610\n",
      "Epoch 34/100, Train Loss: 152.4734, Validation Loss: 163.7185\n",
      "Epoch 35/100, Train Loss: 152.3103, Validation Loss: 163.1744\n",
      "Epoch 36/100, Train Loss: 152.2142, Validation Loss: 162.5177\n",
      "Epoch 37/100, Train Loss: 151.8424, Validation Loss: 162.6126\n",
      "Epoch 38/100, Train Loss: 151.5532, Validation Loss: 163.1012\n",
      "Epoch 39/100, Train Loss: 151.3736, Validation Loss: 162.9875\n",
      "Epoch 40/100, Train Loss: 150.7207, Validation Loss: 163.9191\n",
      "Epoch 41/100, Train Loss: 150.1388, Validation Loss: 161.8685\n",
      "Epoch 42/100, Train Loss: 150.5300, Validation Loss: 161.9950\n",
      "Epoch 43/100, Train Loss: 150.0266, Validation Loss: 162.3167\n",
      "Epoch 44/100, Train Loss: 148.8308, Validation Loss: 163.2495\n",
      "Epoch 45/100, Train Loss: 149.0426, Validation Loss: 161.6270\n",
      "Epoch 46/100, Train Loss: 148.9670, Validation Loss: 161.9983\n",
      "Epoch 47/100, Train Loss: 148.4293, Validation Loss: 161.9395\n",
      "Epoch 48/100, Train Loss: 148.2451, Validation Loss: 162.0951\n",
      "Epoch 49/100, Train Loss: 147.7660, Validation Loss: 163.6623\n",
      "Epoch 50/100, Train Loss: 147.5189, Validation Loss: 161.5994\n",
      "Epoch 51/100, Train Loss: 147.1693, Validation Loss: 161.6779\n",
      "Epoch 52/100, Train Loss: 146.2222, Validation Loss: 161.7942\n",
      "Epoch 53/100, Train Loss: 146.1751, Validation Loss: 165.0428\n",
      "Epoch 54/100, Train Loss: 145.9923, Validation Loss: 161.4945\n",
      "Epoch 55/100, Train Loss: 145.3582, Validation Loss: 163.1102\n",
      "Epoch 56/100, Train Loss: 145.3060, Validation Loss: 162.1199\n",
      "Epoch 57/100, Train Loss: 145.0608, Validation Loss: 162.5578\n",
      "Epoch 58/100, Train Loss: 144.6184, Validation Loss: 162.0586\n",
      "Epoch 59/100, Train Loss: 144.8073, Validation Loss: 161.4310\n",
      "Epoch 60/100, Train Loss: 144.5452, Validation Loss: 161.3086\n",
      "Epoch 61/100, Train Loss: 144.1413, Validation Loss: 161.6731\n",
      "Epoch 62/100, Train Loss: 143.2955, Validation Loss: 161.2534\n",
      "Epoch 63/100, Train Loss: 143.0922, Validation Loss: 163.0181\n",
      "Epoch 64/100, Train Loss: 143.1473, Validation Loss: 161.4862\n",
      "Epoch 65/100, Train Loss: 142.5314, Validation Loss: 161.4360\n",
      "Epoch 66/100, Train Loss: 142.3354, Validation Loss: 160.8480\n",
      "Epoch 67/100, Train Loss: 142.4831, Validation Loss: 160.5365\n",
      "Epoch 68/100, Train Loss: 141.9735, Validation Loss: 160.8810\n",
      "Epoch 69/100, Train Loss: 141.7880, Validation Loss: 160.9933\n",
      "Epoch 70/100, Train Loss: 141.4126, Validation Loss: 160.9155\n",
      "Epoch 71/100, Train Loss: 141.0943, Validation Loss: 161.5027\n",
      "Epoch 72/100, Train Loss: 140.4328, Validation Loss: 161.7043\n",
      "Epoch 73/100, Train Loss: 140.4488, Validation Loss: 161.4655\n",
      "Epoch 74/100, Train Loss: 140.0717, Validation Loss: 161.6634\n",
      "Epoch 75/100, Train Loss: 139.9787, Validation Loss: 161.0153\n",
      "Epoch 76/100, Train Loss: 139.2644, Validation Loss: 160.1268\n",
      "Epoch 77/100, Train Loss: 138.7007, Validation Loss: 161.2475\n",
      "Epoch 78/100, Train Loss: 139.4120, Validation Loss: 160.5486\n",
      "Epoch 79/100, Train Loss: 138.1984, Validation Loss: 160.6562\n",
      "Epoch 80/100, Train Loss: 138.4717, Validation Loss: 162.2231\n",
      "Epoch 81/100, Train Loss: 138.4226, Validation Loss: 161.4767\n",
      "Epoch 82/100, Train Loss: 137.6795, Validation Loss: 160.5077\n",
      "Epoch 83/100, Train Loss: 137.2378, Validation Loss: 161.9036\n",
      "Epoch 84/100, Train Loss: 137.1697, Validation Loss: 160.7262\n",
      "Epoch 85/100, Train Loss: 136.6929, Validation Loss: 161.4637\n",
      "Epoch 86/100, Train Loss: 136.5247, Validation Loss: 161.5026\n",
      "Epoch 87/100, Train Loss: 135.9451, Validation Loss: 160.2896\n",
      "Epoch 88/100, Train Loss: 136.2545, Validation Loss: 161.9235\n",
      "Epoch 89/100, Train Loss: 135.7956, Validation Loss: 160.8824\n",
      "Epoch 90/100, Train Loss: 135.9892, Validation Loss: 160.8211\n",
      "Epoch 91/100, Train Loss: 135.2855, Validation Loss: 160.7911\n",
      "Epoch 92/100, Train Loss: 135.0993, Validation Loss: 161.1574\n",
      "Epoch 93/100, Train Loss: 134.4431, Validation Loss: 162.2695\n",
      "Epoch 94/100, Train Loss: 134.3332, Validation Loss: 161.6593\n",
      "Epoch 95/100, Train Loss: 134.1491, Validation Loss: 161.2693\n",
      "Epoch 96/100, Train Loss: 134.1459, Validation Loss: 162.1369\n",
      "Epoch 97/100, Train Loss: 134.2123, Validation Loss: 161.2706\n",
      "Epoch 98/100, Train Loss: 133.0351, Validation Loss: 161.1184\n",
      "Epoch 99/100, Train Loss: 133.5156, Validation Loss: 161.1811\n",
      "Epoch 100/100, Train Loss: 132.4300, Validation Loss: 161.4358\n",
      "Fold 4/5\n",
      "Epoch 1/100, Train Loss: 1590.0296, Validation Loss: 755.4336\n",
      "Epoch 2/100, Train Loss: 440.7480, Validation Loss: 239.4318\n",
      "Epoch 3/100, Train Loss: 193.9703, Validation Loss: 167.8618\n",
      "Epoch 4/100, Train Loss: 169.6225, Validation Loss: 164.7183\n",
      "Epoch 5/100, Train Loss: 169.1836, Validation Loss: 164.7016\n",
      "Epoch 6/100, Train Loss: 168.9307, Validation Loss: 164.6912\n",
      "Epoch 7/100, Train Loss: 168.5208, Validation Loss: 164.6956\n",
      "Epoch 8/100, Train Loss: 169.0224, Validation Loss: 164.2290\n",
      "Epoch 9/100, Train Loss: 167.8362, Validation Loss: 163.0081\n",
      "Epoch 10/100, Train Loss: 166.2597, Validation Loss: 162.3974\n",
      "Epoch 11/100, Train Loss: 165.9674, Validation Loss: 161.0406\n",
      "Epoch 12/100, Train Loss: 165.0670, Validation Loss: 160.7132\n",
      "Epoch 13/100, Train Loss: 164.5024, Validation Loss: 160.2653\n",
      "Epoch 14/100, Train Loss: 163.3867, Validation Loss: 159.3234\n",
      "Epoch 15/100, Train Loss: 163.1004, Validation Loss: 159.2094\n",
      "Epoch 16/100, Train Loss: 162.5999, Validation Loss: 159.1780\n",
      "Epoch 17/100, Train Loss: 161.6552, Validation Loss: 158.7890\n",
      "Epoch 18/100, Train Loss: 161.1066, Validation Loss: 158.0865\n",
      "Epoch 19/100, Train Loss: 160.5904, Validation Loss: 157.9591\n",
      "Epoch 20/100, Train Loss: 160.2940, Validation Loss: 157.7168\n",
      "Epoch 21/100, Train Loss: 159.9928, Validation Loss: 156.7402\n",
      "Epoch 22/100, Train Loss: 158.9259, Validation Loss: 157.3735\n",
      "Epoch 23/100, Train Loss: 158.3075, Validation Loss: 157.1504\n",
      "Epoch 24/100, Train Loss: 158.1612, Validation Loss: 157.1704\n",
      "Epoch 25/100, Train Loss: 158.0013, Validation Loss: 156.6667\n",
      "Epoch 26/100, Train Loss: 157.6587, Validation Loss: 157.0556\n",
      "Epoch 27/100, Train Loss: 156.7269, Validation Loss: 156.3149\n",
      "Epoch 28/100, Train Loss: 156.1878, Validation Loss: 157.4426\n",
      "Epoch 29/100, Train Loss: 156.5261, Validation Loss: 156.0437\n",
      "Epoch 30/100, Train Loss: 155.6414, Validation Loss: 156.0128\n",
      "Epoch 31/100, Train Loss: 155.4458, Validation Loss: 155.6436\n",
      "Epoch 32/100, Train Loss: 155.7060, Validation Loss: 156.1860\n",
      "Epoch 33/100, Train Loss: 154.7956, Validation Loss: 156.4203\n",
      "Epoch 34/100, Train Loss: 154.1841, Validation Loss: 155.3262\n",
      "Epoch 35/100, Train Loss: 154.3248, Validation Loss: 158.1224\n",
      "Epoch 36/100, Train Loss: 154.1333, Validation Loss: 155.6870\n",
      "Epoch 37/100, Train Loss: 153.4703, Validation Loss: 155.9304\n",
      "Epoch 38/100, Train Loss: 153.0386, Validation Loss: 154.0752\n",
      "Epoch 39/100, Train Loss: 153.1901, Validation Loss: 154.9245\n",
      "Epoch 40/100, Train Loss: 152.0496, Validation Loss: 153.9426\n",
      "Epoch 41/100, Train Loss: 151.6649, Validation Loss: 155.3423\n",
      "Epoch 42/100, Train Loss: 151.5142, Validation Loss: 153.8704\n",
      "Epoch 43/100, Train Loss: 151.4886, Validation Loss: 155.8186\n",
      "Epoch 44/100, Train Loss: 151.0941, Validation Loss: 154.1268\n",
      "Epoch 45/100, Train Loss: 150.5431, Validation Loss: 153.9372\n",
      "Epoch 46/100, Train Loss: 150.0939, Validation Loss: 153.8745\n",
      "Epoch 47/100, Train Loss: 149.4272, Validation Loss: 154.1196\n",
      "Epoch 48/100, Train Loss: 149.9313, Validation Loss: 152.9708\n",
      "Epoch 49/100, Train Loss: 149.4108, Validation Loss: 155.6522\n",
      "Epoch 50/100, Train Loss: 149.0730, Validation Loss: 154.3508\n",
      "Epoch 51/100, Train Loss: 148.9040, Validation Loss: 154.2627\n",
      "Epoch 52/100, Train Loss: 148.0355, Validation Loss: 153.6777\n",
      "Epoch 53/100, Train Loss: 148.4316, Validation Loss: 153.0422\n",
      "Epoch 54/100, Train Loss: 147.2534, Validation Loss: 154.8765\n",
      "Epoch 55/100, Train Loss: 147.0898, Validation Loss: 153.3843\n",
      "Epoch 56/100, Train Loss: 146.6864, Validation Loss: 154.5424\n",
      "Epoch 57/100, Train Loss: 146.9162, Validation Loss: 153.7815\n",
      "Epoch 58/100, Train Loss: 146.5807, Validation Loss: 153.4339\n",
      "Epoch 59/100, Train Loss: 145.8638, Validation Loss: 153.7277\n",
      "Epoch 60/100, Train Loss: 145.8254, Validation Loss: 153.3685\n",
      "Epoch 61/100, Train Loss: 145.5909, Validation Loss: 154.9149\n",
      "Epoch 62/100, Train Loss: 144.7920, Validation Loss: 152.3527\n",
      "Epoch 63/100, Train Loss: 144.3100, Validation Loss: 155.0968\n",
      "Epoch 64/100, Train Loss: 144.8488, Validation Loss: 154.6309\n",
      "Epoch 65/100, Train Loss: 144.2051, Validation Loss: 154.3575\n",
      "Epoch 66/100, Train Loss: 143.8063, Validation Loss: 153.1129\n",
      "Epoch 67/100, Train Loss: 143.2024, Validation Loss: 153.4883\n",
      "Epoch 68/100, Train Loss: 142.7558, Validation Loss: 152.4191\n",
      "Epoch 69/100, Train Loss: 142.4969, Validation Loss: 153.0531\n",
      "Epoch 70/100, Train Loss: 142.2810, Validation Loss: 154.2932\n",
      "Epoch 71/100, Train Loss: 142.1058, Validation Loss: 155.8936\n",
      "Epoch 72/100, Train Loss: 141.4090, Validation Loss: 153.9445\n",
      "Epoch 73/100, Train Loss: 141.4380, Validation Loss: 154.7656\n",
      "Epoch 74/100, Train Loss: 140.9614, Validation Loss: 152.9304\n",
      "Epoch 75/100, Train Loss: 140.8164, Validation Loss: 153.4831\n",
      "Epoch 76/100, Train Loss: 140.5692, Validation Loss: 154.7585\n",
      "Epoch 77/100, Train Loss: 139.8841, Validation Loss: 153.3805\n",
      "Epoch 78/100, Train Loss: 139.8420, Validation Loss: 153.5215\n",
      "Epoch 79/100, Train Loss: 139.4679, Validation Loss: 158.7831\n",
      "Epoch 80/100, Train Loss: 139.7962, Validation Loss: 153.7908\n",
      "Epoch 81/100, Train Loss: 139.0907, Validation Loss: 153.3537\n",
      "Epoch 82/100, Train Loss: 138.9455, Validation Loss: 154.5311\n",
      "Epoch 83/100, Train Loss: 137.9665, Validation Loss: 154.7443\n",
      "Epoch 84/100, Train Loss: 137.8083, Validation Loss: 155.6109\n",
      "Epoch 85/100, Train Loss: 137.6088, Validation Loss: 155.5503\n",
      "Epoch 86/100, Train Loss: 137.4010, Validation Loss: 153.8072\n",
      "Epoch 87/100, Train Loss: 136.9018, Validation Loss: 154.6028\n",
      "Epoch 88/100, Train Loss: 136.7134, Validation Loss: 153.4770\n",
      "Epoch 89/100, Train Loss: 136.2860, Validation Loss: 153.9717\n",
      "Epoch 90/100, Train Loss: 136.0027, Validation Loss: 154.8226\n",
      "Epoch 91/100, Train Loss: 136.2935, Validation Loss: 156.3390\n",
      "Epoch 92/100, Train Loss: 135.8861, Validation Loss: 153.1207\n",
      "Epoch 93/100, Train Loss: 135.3481, Validation Loss: 153.4243\n",
      "Epoch 94/100, Train Loss: 134.9870, Validation Loss: 154.9173\n",
      "Epoch 95/100, Train Loss: 135.0974, Validation Loss: 154.0241\n",
      "Epoch 96/100, Train Loss: 134.2149, Validation Loss: 154.7516\n",
      "Epoch 97/100, Train Loss: 134.4086, Validation Loss: 154.3916\n",
      "Epoch 98/100, Train Loss: 134.7360, Validation Loss: 153.2029\n",
      "Epoch 99/100, Train Loss: 133.4427, Validation Loss: 153.4565\n",
      "Epoch 100/100, Train Loss: 132.9964, Validation Loss: 154.8468\n",
      "Fold 5/5\n",
      "Epoch 1/100, Train Loss: 1659.1538, Validation Loss: 848.5796\n",
      "Epoch 2/100, Train Loss: 460.8559, Validation Loss: 240.4831\n",
      "Epoch 3/100, Train Loss: 194.3864, Validation Loss: 169.7988\n",
      "Epoch 4/100, Train Loss: 169.2254, Validation Loss: 166.2205\n",
      "Epoch 5/100, Train Loss: 168.5908, Validation Loss: 166.1874\n",
      "Epoch 6/100, Train Loss: 168.3389, Validation Loss: 166.0856\n",
      "Epoch 7/100, Train Loss: 168.3934, Validation Loss: 165.8653\n",
      "Epoch 8/100, Train Loss: 167.2944, Validation Loss: 164.9493\n",
      "Epoch 9/100, Train Loss: 166.5379, Validation Loss: 164.5208\n",
      "Epoch 10/100, Train Loss: 165.8681, Validation Loss: 164.2328\n",
      "Epoch 11/100, Train Loss: 164.7729, Validation Loss: 162.8703\n",
      "Epoch 12/100, Train Loss: 164.1303, Validation Loss: 162.5653\n",
      "Epoch 13/100, Train Loss: 163.4083, Validation Loss: 161.5370\n",
      "Epoch 14/100, Train Loss: 162.8917, Validation Loss: 161.4738\n",
      "Epoch 15/100, Train Loss: 162.4969, Validation Loss: 160.7334\n",
      "Epoch 16/100, Train Loss: 161.3992, Validation Loss: 160.2513\n",
      "Epoch 17/100, Train Loss: 161.3966, Validation Loss: 159.8124\n",
      "Epoch 18/100, Train Loss: 160.8267, Validation Loss: 160.0408\n",
      "Epoch 19/100, Train Loss: 159.8114, Validation Loss: 159.6037\n",
      "Epoch 20/100, Train Loss: 159.4854, Validation Loss: 159.3462\n",
      "Epoch 21/100, Train Loss: 158.8657, Validation Loss: 159.6924\n",
      "Epoch 22/100, Train Loss: 158.6465, Validation Loss: 158.9143\n",
      "Epoch 23/100, Train Loss: 158.1965, Validation Loss: 159.2208\n",
      "Epoch 24/100, Train Loss: 157.7309, Validation Loss: 158.4503\n",
      "Epoch 25/100, Train Loss: 157.1510, Validation Loss: 158.8190\n",
      "Epoch 26/100, Train Loss: 156.7420, Validation Loss: 158.0479\n",
      "Epoch 27/100, Train Loss: 156.5923, Validation Loss: 158.1593\n",
      "Epoch 28/100, Train Loss: 156.2101, Validation Loss: 158.1019\n",
      "Epoch 29/100, Train Loss: 155.8803, Validation Loss: 157.3314\n",
      "Epoch 30/100, Train Loss: 155.4270, Validation Loss: 157.3429\n",
      "Epoch 31/100, Train Loss: 155.6408, Validation Loss: 157.6260\n",
      "Epoch 32/100, Train Loss: 155.6062, Validation Loss: 158.2113\n",
      "Epoch 33/100, Train Loss: 155.1017, Validation Loss: 157.7335\n",
      "Epoch 34/100, Train Loss: 154.4500, Validation Loss: 157.0535\n",
      "Epoch 35/100, Train Loss: 154.1191, Validation Loss: 158.3331\n",
      "Epoch 36/100, Train Loss: 153.6580, Validation Loss: 156.2474\n",
      "Epoch 37/100, Train Loss: 153.7166, Validation Loss: 156.7113\n",
      "Epoch 38/100, Train Loss: 153.1095, Validation Loss: 156.5228\n",
      "Epoch 39/100, Train Loss: 153.0680, Validation Loss: 156.5714\n",
      "Epoch 40/100, Train Loss: 152.9940, Validation Loss: 157.4491\n",
      "Epoch 41/100, Train Loss: 152.3601, Validation Loss: 156.5024\n",
      "Epoch 42/100, Train Loss: 152.0488, Validation Loss: 157.1265\n",
      "Epoch 43/100, Train Loss: 151.8774, Validation Loss: 156.2671\n",
      "Epoch 44/100, Train Loss: 150.9968, Validation Loss: 157.1809\n",
      "Epoch 45/100, Train Loss: 150.8417, Validation Loss: 155.9424\n",
      "Epoch 46/100, Train Loss: 151.0240, Validation Loss: 155.2643\n",
      "Epoch 47/100, Train Loss: 150.1750, Validation Loss: 155.2486\n",
      "Epoch 48/100, Train Loss: 150.1111, Validation Loss: 156.0510\n",
      "Epoch 49/100, Train Loss: 150.2591, Validation Loss: 155.6978\n",
      "Epoch 50/100, Train Loss: 149.6393, Validation Loss: 156.0095\n",
      "Epoch 51/100, Train Loss: 150.0628, Validation Loss: 155.9938\n",
      "Epoch 52/100, Train Loss: 148.6031, Validation Loss: 157.0790\n",
      "Epoch 53/100, Train Loss: 148.4349, Validation Loss: 156.2547\n",
      "Epoch 54/100, Train Loss: 148.2126, Validation Loss: 156.7667\n",
      "Epoch 55/100, Train Loss: 147.8820, Validation Loss: 156.3753\n",
      "Epoch 56/100, Train Loss: 148.0574, Validation Loss: 155.4650\n",
      "Epoch 57/100, Train Loss: 147.3971, Validation Loss: 156.4951\n",
      "Epoch 58/100, Train Loss: 147.1182, Validation Loss: 155.8052\n",
      "Epoch 59/100, Train Loss: 146.8159, Validation Loss: 154.8600\n",
      "Epoch 60/100, Train Loss: 146.8440, Validation Loss: 155.6511\n",
      "Epoch 61/100, Train Loss: 146.1686, Validation Loss: 155.1779\n",
      "Epoch 62/100, Train Loss: 145.9707, Validation Loss: 155.4891\n",
      "Epoch 63/100, Train Loss: 145.6952, Validation Loss: 155.5799\n",
      "Epoch 64/100, Train Loss: 145.3223, Validation Loss: 155.3720\n",
      "Epoch 65/100, Train Loss: 145.3679, Validation Loss: 155.5685\n",
      "Epoch 66/100, Train Loss: 144.8771, Validation Loss: 155.0025\n",
      "Epoch 67/100, Train Loss: 144.3418, Validation Loss: 155.3562\n",
      "Epoch 68/100, Train Loss: 144.2459, Validation Loss: 155.7495\n",
      "Epoch 69/100, Train Loss: 144.2730, Validation Loss: 155.6337\n",
      "Epoch 70/100, Train Loss: 143.2932, Validation Loss: 155.1208\n",
      "Epoch 71/100, Train Loss: 143.5817, Validation Loss: 154.7255\n",
      "Epoch 72/100, Train Loss: 143.3357, Validation Loss: 155.4665\n",
      "Epoch 73/100, Train Loss: 142.9903, Validation Loss: 154.6403\n",
      "Epoch 74/100, Train Loss: 143.2207, Validation Loss: 155.0678\n",
      "Epoch 75/100, Train Loss: 142.1335, Validation Loss: 155.3607\n",
      "Epoch 76/100, Train Loss: 142.0719, Validation Loss: 154.8591\n",
      "Epoch 77/100, Train Loss: 158.4516, Validation Loss: 160.6542\n",
      "Epoch 78/100, Train Loss: 159.6888, Validation Loss: 159.7146\n",
      "Epoch 79/100, Train Loss: 158.7157, Validation Loss: 159.0712\n",
      "Epoch 80/100, Train Loss: 158.2170, Validation Loss: 158.6847\n",
      "Epoch 81/100, Train Loss: 157.6724, Validation Loss: 158.4343\n",
      "Epoch 82/100, Train Loss: 157.6054, Validation Loss: 158.3827\n",
      "Epoch 83/100, Train Loss: 157.2030, Validation Loss: 158.0451\n",
      "Epoch 84/100, Train Loss: 157.1748, Validation Loss: 158.4254\n",
      "Epoch 85/100, Train Loss: 156.7275, Validation Loss: 158.5668\n",
      "Epoch 86/100, Train Loss: 156.2031, Validation Loss: 157.6821\n",
      "Epoch 87/100, Train Loss: 156.0802, Validation Loss: 157.9853\n",
      "Epoch 88/100, Train Loss: 155.5256, Validation Loss: 156.9821\n",
      "Epoch 89/100, Train Loss: 154.9868, Validation Loss: 156.7756\n",
      "Epoch 90/100, Train Loss: 154.7060, Validation Loss: 157.6259\n",
      "Epoch 91/100, Train Loss: 154.6175, Validation Loss: 157.8973\n",
      "Epoch 92/100, Train Loss: 154.1193, Validation Loss: 159.1125\n",
      "Epoch 93/100, Train Loss: 152.7499, Validation Loss: 158.5154\n",
      "Epoch 94/100, Train Loss: 152.1582, Validation Loss: 157.5666\n",
      "Epoch 95/100, Train Loss: 151.3748, Validation Loss: 157.8332\n",
      "Epoch 96/100, Train Loss: 150.5174, Validation Loss: 157.0726\n",
      "Epoch 97/100, Train Loss: 150.3009, Validation Loss: 157.7905\n",
      "Epoch 98/100, Train Loss: 149.8858, Validation Loss: 157.7515\n",
      "Epoch 99/100, Train Loss: 149.1068, Validation Loss: 156.7852\n",
      "Epoch 100/100, Train Loss: 148.6803, Validation Loss: 157.0273\n",
      "\n",
      "K-Fold Cross-Validation Results:\n",
      "Average Validation Loss: 157.2932\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "\n",
    "collate_fn = lambda batch: (\n",
    "    nn.utils.rnn.pad_sequence([item[0] for item in batch], batch_first=True),\n",
    "    torch.tensor([len(item[0]) for item in batch]),\n",
    "    torch.tensor([item[1] for item in batch], dtype=torch.float32)\n",
    ")\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)  # LSTM layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)  # Fully connected layer for output\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        # Pack the padded sequences\n",
    "        packed_input = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hidden, _) = self.rnn(packed_input)  # LSTM output\n",
    "        \n",
    "        # Use the last hidden state\n",
    "        output = self.fc(hidden[-1])  # Take the last layer of the hidden state\n",
    "        return output\n",
    "\n",
    "# Model hyperparameters\n",
    "input_size = input[0].shape[1]  # Number of features\n",
    "hidden_size = 64\n",
    "output_size = 1  # Regression target\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "k = 5  # Number of folds\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "predictions_per_fold = []\n",
    "labels_per_fold = []\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f\"Fold {fold+1}/{k}\")\n",
    "    \n",
    "    # Create train and validation subsets\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Initialize model and optimizer\n",
    "    model = RNNModel(input_size, hidden_size, output_size).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Train and validate\n",
    "    epochs = 100\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_X, lengths, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            predictions = model(batch_X, lengths)\n",
    "            loss = loss_fn(predictions, batch_y.unsqueeze(1))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, lengths, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                predictions = model(batch_X, lengths)\n",
    "                loss = loss_fn(predictions, batch_y.unsqueeze(1))\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Validation Loss: {val_loss/len(val_loader):.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, lengths, batch_y in val_loader:\n",
    "            predictions = model(batch_X.to(device), lengths).to(\"cpu\")\n",
    "            loss = loss_fn(predictions, batch_y.unsqueeze(1))\n",
    "            val_loss += loss.item()\n",
    "            all_predictions.append(predictions)\n",
    "            all_labels.append(batch_y)\n",
    "\n",
    "    # Concatenate all predictions and true labels\n",
    "    all_predictions = torch.cat(all_predictions, dim=0).tolist()\n",
    "    all_labels = torch.cat(all_labels, dim=0).tolist()\n",
    "\n",
    "    predictions_per_fold.append(all_predictions)\n",
    "    labels_per_fold.append(all_labels)\n",
    "\n",
    "    # Save results for this fold\n",
    "    fold_results.append({\n",
    "        \"fold\": fold + 1,\n",
    "        \"final_validation_loss\": val_losses[-1]\n",
    "    })\n",
    "\n",
    "# Aggregate fold results\n",
    "avg_loss = sum([result[\"final_validation_loss\"] for result in fold_results]) / k\n",
    "print(f\"\\nK-Fold Cross-Validation Results:\\nAverage Validation Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results = {}\n",
    "for i in range(k):\n",
    "    results[f\"tm_{i}\"] = labels_per_fold[i]\n",
    "    results[f\"preds_{i}\"] = [item[0] for item in predictions_per_fold[i]]\n",
    "\n",
    "pd.DataFrame(results).to_csv(\"../predictions/carbonara_rnn_z.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
